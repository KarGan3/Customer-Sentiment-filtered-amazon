# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10fStAS3uMZthQdNYne7RtJAxXKIQLgku
"""

import pandas as pd
df = pd.read_csv('/content/sample_data/Customer_Sentiment_filtered_amazon.csv')
print(df.head().to_markdown(index=False))

import pandas as pd
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC

# 1. Load the dataset (from f653c5b7)
df = pd.read_csv('/content/sample_data/Customer_Sentiment_filtered_amazon.csv')

# 2. Data Preparation (from c875a220)
X = df['review_text'] # Corrected column name
y = df['sentiment'] # Corrected column name

label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

tfidf_vectorizer = TfidfVectorizer()
X_vectorized = tfidf_vectorizer.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)

if hasattr(X_train, 'toarray'):
    X_train_dense = X_train.toarray()
else:
    X_train_dense = X_train

if hasattr(X_test, 'toarray'):
    X_test_dense = X_test.toarray()
else:
    X_test_dense = X_test

scaler = StandardScaler(with_mean=False)
X_train = scaler.fit_transform(X_train_dense)
X_test = scaler.transform(X_test_dense)

print("Data preparation complete:")
print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_test shape: {y_test.shape}")
print(f"First 5 target labels (encoded): {y[:5]}")
print(f"Number of unique target classes: {len(label_encoder.classes_)}")
print(f"Original target classes: {label_encoder.classes_}")

# 3. Train the SVM model (from 9435b4b1)
svm_model = SVC(kernel='linear', C=0.1, random_state=42) # Added regularization parameter C
svm_model.fit(X_train, y_train)

print("SVM model trained successfully.")

from sklearn.metrics import accuracy_score, classification_report

# Make predictions on the test set
y_pred = svm_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)

print(f"Model Accuracy: {accuracy:.4f}")

from sklearn.metrics import accuracy_score, classification_report

# Make predictions on the test set
y_pred = svm_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred, target_names=label_encoder.classes_, zero_division=0)

print(f"Model Accuracy: {accuracy:.4f}")
print("\nClassification Report:")
print(report)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Generate the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

def predict_sentiment(text_input):
    text_vectorized = tfidf_vectorizer.transform([text_input])

    if hasattr(text_vectorized, 'toarray'):
        text_dense = text_vectorized.toarray()
    else:
        text_dense = text_vectorized

    text_scaled = scaler.transform(text_dense)

    prediction = svm_model.predict(text_scaled)

    predicted_sentiment = label_encoder.inverse_transform(prediction)

    return predicted_sentiment[0]


print("Sentiment prediction function created.")

# Example usage:
user_input = input("Enter a product summary to predict its sentiment: ")
predicted_sentiment = predict_sentiment(user_input)
print(f"The predicted sentiment for '{user_input}' is: {predicted_sentiment}")

"""### Notebook Cell Explanations

*   **Cell `f653c5b7`**: Loads the `Customer_Sentiment_filtered_amazon.csv` dataset into a pandas DataFrame and displays its first 5 rows.
*   **Cell `89269634`**: This cell now combines multiple steps:
    *   Loads the dataset again (to ensure `df` is defined).
    *   Prepares the data: extracts `review_text` and `sentiment`, encodes sentiment labels, vectorizes text using TF-IDF, splits data into training/testing sets, and scales the features.
    *   Trains an SVM (Support Vector Machine) model using the prepared training data.
*   **Cell `5dc8c084`**: Evaluates the trained SVM model by making predictions on the test set and calculating accuracy and a classification report.
*   **Cell `2733dda7`**: Similar to `5dc8c084`, it evaluates the model and provides a classification report, specifically handling `zero_division` to avoid warnings.
*   **Cell `05336229`**: Visualizes the model's performance using a confusion matrix, plotting it as a heatmap.
*   **Cell `bf565660`**: Defines the `predict_sentiment` function, which takes a raw text input, processes it through the same TF-IDF vectorizer and scaler, and then uses the trained SVM model to predict and return its sentiment.
*   **Cell `b047a2e3`**: Demonstrates the usage of the `predict_sentiment` function by prompting the user for input and displaying the predicted sentiment.

# Task
Improve the existing sentiment analysis SVM model by performing hyperparameter tuning to optimize its performance on unseen data, and then re-evaluate the model's predictions on these unseen examples to demonstrate improvement.

## Diagnose Current Model Performance on Unseen Data

### Subtask:
Evaluate the current SVM model's performance on specific unseen examples to establish a baseline before hyperparameter tuning.

**Reasoning**:
I will create a list of unseen reviews and their corresponding expected sentiments. Then, I will iterate through these reviews, use the `predict_sentiment` function to get the model's prediction, and print the original review, expected sentiment, and predicted sentiment to evaluate the model's performance on these specific examples.
"""

unseen_reviews = [
    "This is an amazing product, I love it!",
    "The delivery was delayed and the item was broken.",
    "It's okay, nothing special.",
    "Absolutely terrible experience, never buying again.",
    "Good enough for the price."
]
expected_sentiments = [
    "positive",
    "negative",
    "neutral",
    "negative",
    "neutral"
]

print("\n--- Evaluating Model Performance on Unseen Examples ---")
for i, review in enumerate(unseen_reviews):
    predicted_sentiment = predict_sentiment(review)
    print(f"Review: '{review}'")
    print(f"Expected: {expected_sentiments[i]}, Predicted: {predicted_sentiment}")
    print("----------------------------------------------------")

"""### Current Model Performance Baseline on Unseen Examples

The evaluation of the current SVM model on specific unseen examples reveals the following:

*   **Correct Predictions**:
    *   `'This is an amazing product, I love it!'`: Predicted **positive** (Correct)
    *   `'The delivery was delayed and the item was broken.'`: Predicted **negative** (Correct)
    *   `'It's okay, nothing special.'`: Predicted **neutral** (Correct)

*   **Incorrect Predictions (Misclassifications)**:
    *   `'Absolutely terrible experience, never buying again.'`: Expected **negative**, Predicted **positive** (Incorrect)
    *   `'Good enough for the price.'`: Expected **neutral**, Predicted **positive** (Incorrect)

These results establish a baseline. The model correctly classifies 3 out of 5 unseen examples but struggles with nuanced negative and neutral sentiments, misclassifying them as positive. This indicates a potential area for improvement through hyperparameter tuning.

## Implement Hyperparameter Tuning for SVM

### Subtask:
To improve the model's ability to generalize, we will perform hyperparameter tuning using GridSearchCV to find the optimal 'C' regularization parameter for the SVM. This will help prevent overfitting and potentially improve predictions on new data. The tuning process will involve retraining the model multiple times with different parameter values.

**Reasoning**:
To perform hyperparameter tuning for the SVM model, I will use `GridSearchCV` to explore different values for the `C` regularization parameter. This will involve defining the parameter grid, initializing `GridSearchCV`, fitting it to the training data, and then extracting the best parameters, best score, and the best estimator.
"""

from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC

# 2. Define the parameter grid
param_grid = {
    'C': [0.01, 0.1, 1, 10, 100]
}

# 3. Initialize GridSearchCV
# The SVC model in GridSearchCV should also be initialized with kernel='linear' and random_state=42
grid_search = GridSearchCV(SVC(kernel='linear', random_state=42), param_grid, cv=5, scoring='accuracy', n_jobs=-1) # Use n_jobs=-1 to parallelize

# 4. Fit GridSearchCV to the training data
grid_search.fit(X_train, y_train)

# 5. Print the best parameters
print("Best parameters found:", grid_search.best_params_)

# 6. Print the best cross-validation score
print("Best cross-validation accuracy:", grid_search.best_score_)

# 7. Store the best estimator
best_svm_model = grid_search.best_estimator_

print("Hyperparameter tuning complete. Best SVM model stored.")

"""**Reasoning**:
Now that hyperparameter tuning is complete and the `best_svm_model` has been identified, I need to update the `predict_sentiment` function to use this new, optimized model. This will allow for evaluating the tuned model's performance on unseen data.


"""

def predict_sentiment_tuned(text_input):
    text_vectorized = tfidf_vectorizer.transform([text_input])

    if hasattr(text_vectorized, 'toarray'):
        text_dense = text_vectorized.toarray()
    else:
        text_dense = text_vectorized

    text_scaled = scaler.transform(text_dense)

    prediction = best_svm_model.predict(text_scaled)

    predicted_sentiment = label_encoder.inverse_transform(prediction)

    return predicted_sentiment[0]


print("Tuned sentiment prediction function created.")

"""**Reasoning**:
With the `predict_sentiment_tuned` function now defined and using the optimized model, I will re-evaluate the model's performance on the same set of unseen examples to observe any improvements from hyperparameter tuning.


"""

print("\n--- Evaluating Tuned Model Performance on Unseen Examples ---")
for i, review in enumerate(unseen_reviews):
    predicted_sentiment_tuned = predict_sentiment_tuned(review)
    print(f"Review: '{review}'")
    print(f"Expected: {expected_sentiments[i]}, Predicted (Tuned): {predicted_sentiment_tuned}")
    print("----------------------------------------------------")

"""### Tuned Model Performance on Unseen Examples

After hyperparameter tuning, the SVM model with the optimal 'C' value (which was found to be 0.1, same as the initial model's C value) was re-evaluated on the same set of unseen examples.

Here's a comparison:

**Baseline Model Performance (from previous evaluation):**
*   **Correct Predictions (3/5):**
    *   'This is an amazing product, I love it!' (Predicted: positive)
    *   'The delivery was delayed and the item was broken.' (Predicted: negative)
    *   'It's okay, nothing special.' (Predicted: neutral)
*   **Incorrect Predictions (2/5):**
    *   'Absolutely terrible experience, never buying again.' (Expected: negative, Predicted: positive)
    *   'Good enough for the price.' (Expected: neutral, Predicted: positive)

**Tuned Model Performance:**
*   **Correct Predictions (3/5):**
    *   'This is an amazing product, I love it!' (Predicted: positive)
    *   'The delivery was delayed and the item was broken.' (Predicted: negative)
    *   'It's okay, nothing special.' (Predicted: neutral)
*   **Incorrect Predictions (2/5):**
    *   'Absolutely terrible experience, never buying again.' (Expected: negative, Predicted: positive)
    *   'Good enough for the price.' (Expected: neutral, Predicted: positive)

**Conclusion:**

In this specific evaluation with the given unseen examples, the hyperparameter tuning for the `C` parameter did not lead to an observable change in the predictions. The best `C` value identified by `GridSearchCV` was `0.1`, which was already the regularization parameter used in the initial SVM model. This suggests that for these particular unseen examples, the model's performance on nuanced negative and neutral sentiments remains challenging, and further investigation with other hyperparameters (e.g., `gamma` for RBF kernel), more diverse training data, or alternative models might be necessary for improvement.

## Final Task

### Subtask:
Provide a summary of the improvements made to the model's predictions on unseen data and suggest any further steps if necessary.

## Summary:

### Q&A
The hyperparameter tuning for the SVM model's 'C' parameter did not lead to an observable improvement in its predictions on the specific unseen data. The model still correctly classified 3 out of 5 unseen examples and misclassified the same 2 examples as before tuning.

### Data Analysis Key Findings
*   **Initial Model Performance**: The baseline SVM model correctly predicted 3 out of 5 unseen examples, misclassifying two (`'Absolutely terrible experience, never buying again.'` and `'Good enough for the price.'`) as positive when they were expected to be negative and neutral, respectively.
*   **Hyperparameter Tuning Result**: `GridSearchCV` was used to tune the 'C' regularization parameter, and the best 'C' value found was `0.1`, which yielded a best cross-validation accuracy of `0.9796`. This 'C' value was the same as the one used in the initial model.
*   **Tuned Model Performance**: After tuning, the model's predictions on the same set of 5 unseen examples remained identical to the baseline model, with 3 correct predictions and the same 2 misclassifications.

### Insights or Next Steps
*   The lack of improvement after tuning the 'C' parameter suggests that this specific hyperparameter, at least within the tested range and with a linear kernel, was not the primary factor limiting the model's performance on these nuanced examples. This is reinforced by the fact that the optimal 'C' found was already the model's default.
*   To further improve performance, consider tuning other hyperparameters (e.g., `gamma` for non-linear kernels like RBF), exploring different kernels for the SVM, or investigating alternative model architectures. Expanding and diversifying the training data could also help the model better capture complex sentiment nuances.
"""